{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hanlp_restful import HanLPClient\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ast\n",
    "from stylecloud import gen_stylecloud\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = os.listdir(\"data/Interview transcripts-round 3\")\n",
    "file_list = ['1_SZUZJY_来深33年邹先生.docx',\n",
    " '1_SZUZJY_深圳本地人林女士.docx',\n",
    " '2_JNU LLW_DB.docx',\n",
    " '2_JNU LLW_LXS.docx',\n",
    " '2_JNU LLW_RRAY.docx',\n",
    " '2_JNU LLW_SYJZ.docx',\n",
    " '2_JNU LLW_XBB.docx',\n",
    " '3_SZTUWWX_李先生.docx',\n",
    " '3_SZTUWWX_许女士.docx',\n",
    " '3_SZTUWWX_谭女士霍先生.docx',\n",
    " '3_SZTUWWX_邓先生王女士.docx',\n",
    " '3_SZTUWWX_陈女士.docx',\n",
    " '4_UT_LJY_UU.docx',\n",
    " '4_UT_LJY_WW.docx',\n",
    " '4_UT_LJY_XW.docx',\n",
    " '5_CS+刘阿姨.docx',\n",
    " '5_CS+尤老师.docx',\n",
    " '5_CS+钟老师.docx',\n",
    " '5_CS+黄先生.docx',\n",
    " '6_szulys_nw.docx',\n",
    " '6_szulys_乔麦.docx',\n",
    " '7_SZUF_LT.docx',\n",
    " '7_SZUF_坚.docx',\n",
    " '7_SZUF_大花.docx',\n",
    " '8_hkust_wdq_q.docx',\n",
    " '8_hkust_wdq_yyq.docx',\n",
    " '9_SZUCJH_发哥.docx',\n",
    " '9_SZUCJH_小贾.docx',\n",
    " '9_SZUCJH_小陈.docx',\n",
    " '9_SZUCJH_老陈.docx',\n",
    " '10_SZTU-lx_何爷爷.docx',\n",
    " '10_SZTU-lx_凹凸曼&李先生.docx',\n",
    " '11_SZUH_LEI.docx',\n",
    " '11_SZUH_YUNWEI.docx',\n",
    " '12_SZUL_Roson.docx',\n",
    " '12_SZUL_夏天.docx',\n",
    " '12_SZUL_张律师.docx',\n",
    " '12_SZUL_美丽.docx',\n",
    " '12_SZUL_花生.docx',\n",
    " '13_SZTULJK_HSF.docx',\n",
    " '13_SZTULJK_HYZ.docx',\n",
    " '13_SZTULJK_LDH.docx',\n",
    " '13_SZTULJK_LJP.docx',\n",
    " '14_SZUlsr-jiachunxia.docx',\n",
    " '14_SZUlsr-wuxin.docx',\n",
    " '15_PKU+DYE_Suning.docx',\n",
    " '15_PKU+DYE_Xin.docx',\n",
    " '15_PKU+DYE_Xuan.docx',\n",
    " '17_SCNUC_小于先生.docx',\n",
    " '17_SCNUC_小于女士.docx',\n",
    " '18_SZU_XZA_HE.docx',\n",
    " '19_sustech_qk_He.docx',\n",
    " '19_sustech_qk_meng.docx',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HanLP = HanLPClient('https://www.hanlp.com/api', auth='你申请到的auth')  # auth需要申请\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth=\"NTc3MkBiYnMuaGFubHAuY29tOnV6R0xMS05pblB3c29CZE4=\", language='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize(text: Union[str, List[str]], coarse: Optional[bool] = None, language=None) → List[List[str]]\n",
    "# seg = HanLP.tokenize(text)\n",
    "# seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return ''.join(full_text)\n",
    "\n",
    "# def get_location_from_one_hanlp(my_filepath):\n",
    "#     text = read_docx(my_filepath)\n",
    "#     # display(text)\n",
    "\n",
    "#     doc = HanLP(text, tasks='ner/pku', language='zh')\n",
    "#     my_ner_list = doc['ner/pku']\n",
    "\n",
    "#     my_filename = my_filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "#     return my_ner_list, my_filename\n",
    "\n",
    "def split_text(text, max_length=15000):\n",
    "    # 使用\\n分割文本为句子列表\n",
    "    sentences = text.split('\\n')\n",
    "    parts = []\n",
    "    current_part = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # 检查加入当前句子是否会超过最大长度\n",
    "        if len(current_part) + len(sentence) + 1 > max_length:  # +1 是为了计算加入的换行符\n",
    "            # 如果会超过，当前部分结束，开始新的部分\n",
    "            parts.append(current_part)\n",
    "            current_part = sentence  # 开始新的部分，当前句子是新部分的第一句\n",
    "        else:\n",
    "            # 如果不会超过，将当前句子加入当前部分\n",
    "            if current_part:  # 如果当前部分不为空，先加入换行符\n",
    "                current_part += '\\n'\n",
    "            current_part += sentence\n",
    "\n",
    "    # 循环结束后，将最后一部分（如果有）加入到部分列表中\n",
    "    if current_part:\n",
    "        parts.append(current_part)\n",
    "\n",
    "    return parts\n",
    "\n",
    "def get_location_from_one_hanlp(my_filepath):\n",
    "    text = read_docx(my_filepath)\n",
    "    # display(text)\n",
    "\n",
    "    # 检查文本长度并分割\n",
    "    texts = split_text(text) if len(text) > 15000 else [text]\n",
    "\n",
    "    # 初始化空列表来存储所有部分的结果\n",
    "    all_ner_lists = []\n",
    "\n",
    "    # 对每个文本部分调用 API 并合并结果\n",
    "    for part_text in texts:\n",
    "        doc = HanLP(part_text, tasks='ner/pku', language='zh')\n",
    "        my_ner_list = doc['ner/pku']\n",
    "        all_ner_lists.extend(my_ner_list)\n",
    "\n",
    "    my_filename = my_filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    return all_ner_lists, my_filename\n",
    "\n",
    "def get_text_length(my_filepath):\n",
    "    text = read_docx(my_filepath)\n",
    "    # print(len(text))\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file_path = \"data/Interview transcripts-round 3/1_SZUZJY_来深33年邹先生.docx\"\n",
    "# # range_list = [0, 2, 7, 12, 15, 19, 22, 27, 29, 31, 33, 35, 40, 45, 48, 51]\n",
    "# # range_list = range(0, 55)\n",
    "# # range_list = [52]\n",
    "# range_list = range(len(file_list))\n",
    "# for i in tqdm(range_list):\n",
    "#     my_filepath = f\"data/Interview transcripts-round 3/{file_list[i]}\"\n",
    "#     # tmp = split_text(read_docx(my_filepath))\n",
    "#     ner_list, filename = get_location_from_one_hanlp(my_filepath)\n",
    "#     # display(ner_list)\n",
    "#     with open(f\"data/output_hanlp+worldcloud/txt/{filename}.txt\",\"w\") as f:\n",
    "#         f.write(\"[\\n\")\n",
    "#         for item in ner_list:\n",
    "#             f.write(\"%s\\n\" % item)\n",
    "#         f.write(\"]\")\n",
    "#     if i == 25:\n",
    "#         time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert list to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word, we calculate its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_list_todf(my_filename):\n",
    "    with open(f\"data/output_hanlp+worldcloud/txt/{my_filename}.txt\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        tmp_ner_list = [ast.literal_eval(i.strip()) for i in content[1:-1].strip().split(\"\\n\")]\n",
    "    result =[[item[0], (index, item[2],item[3])] for index, sublist in enumerate(tmp_ner_list) for item in sublist if item[1] == 'ns']\n",
    "    df = pd.DataFrame(result, columns=['location_name', 'value']).groupby(\"location_name\").agg({\"value\": lambda x: list(x)}).reset_index()\n",
    "    df[\"count\"] = df[\"value\"].apply(lambda x: len(x))\n",
    "    df[\"filename\"] = my_filename\n",
    "    df.to_csv(f\"data/output_hanlp+worldcloud/csv/{my_filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range_list = range(len(file_list))\n",
    "# for i in tqdm(range_list):\n",
    "#     my_filename = file_list[i][0:-5]\n",
    "#     convert_one_list_todf(my_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all csv files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>value</th>\n",
       "      <th>count</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>上沙</td>\n",
       "      <td>[(70, 10, 11)]</td>\n",
       "      <td>1</td>\n",
       "      <td>10_SZTU-lx_何爷爷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>东莞</td>\n",
       "      <td>[(262, 50, 51)]</td>\n",
       "      <td>1</td>\n",
       "      <td>10_SZTU-lx_何爷爷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>东门</td>\n",
       "      <td>[(63, 6, 7), (211, 47, 48), (212, 3, 4), (227,...</td>\n",
       "      <td>5</td>\n",
       "      <td>10_SZTU-lx_何爷爷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>东门老街</td>\n",
       "      <td>[(8, 6, 8), (8, 9, 11)]</td>\n",
       "      <td>2</td>\n",
       "      <td>10_SZTU-lx_何爷爷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>丹竹头</td>\n",
       "      <td>[(70, 8, 9)]</td>\n",
       "      <td>1</td>\n",
       "      <td>10_SZTU-lx_何爷爷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>香港</td>\n",
       "      <td>[(52, 40, 41)]</td>\n",
       "      <td>1</td>\n",
       "      <td>9_SZUCJH_老陈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>龙华</td>\n",
       "      <td>[(53, 4, 5)]</td>\n",
       "      <td>1</td>\n",
       "      <td>9_SZUCJH_老陈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>龙华区</td>\n",
       "      <td>[(16, 36, 37), (21, 18, 19), (53, 5, 6), (102,...</td>\n",
       "      <td>4</td>\n",
       "      <td>9_SZUCJH_老陈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>龙岗</td>\n",
       "      <td>[(21, 49, 50), (22, 2, 3), (55, 36, 37)]</td>\n",
       "      <td>3</td>\n",
       "      <td>9_SZUCJH_老陈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>龙岗区</td>\n",
       "      <td>[(16, 42, 43)]</td>\n",
       "      <td>1</td>\n",
       "      <td>9_SZUCJH_老陈</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2603 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name                                              value  count  \\\n",
       "0               上沙                                     [(70, 10, 11)]      1   \n",
       "1               东莞                                    [(262, 50, 51)]      1   \n",
       "2               东门  [(63, 6, 7), (211, 47, 48), (212, 3, 4), (227,...      5   \n",
       "3             东门老街                            [(8, 6, 8), (8, 9, 11)]      2   \n",
       "4              丹竹头                                       [(70, 8, 9)]      1   \n",
       "...            ...                                                ...    ...   \n",
       "2598            香港                                     [(52, 40, 41)]      1   \n",
       "2599            龙华                                       [(53, 4, 5)]      1   \n",
       "2600           龙华区  [(16, 36, 37), (21, 18, 19), (53, 5, 6), (102,...      4   \n",
       "2601            龙岗           [(21, 49, 50), (22, 2, 3), (55, 36, 37)]      3   \n",
       "2602           龙岗区                                     [(16, 42, 43)]      1   \n",
       "\n",
       "            filename  \n",
       "0     10_SZTU-lx_何爷爷  \n",
       "1     10_SZTU-lx_何爷爷  \n",
       "2     10_SZTU-lx_何爷爷  \n",
       "3     10_SZTU-lx_何爷爷  \n",
       "4     10_SZTU-lx_何爷爷  \n",
       "...              ...  \n",
       "2598     9_SZUCJH_老陈  \n",
       "2599     9_SZUCJH_老陈  \n",
       "2600     9_SZUCJH_老陈  \n",
       "2601     9_SZUCJH_老陈  \n",
       "2602     9_SZUCJH_老陈  \n",
       "\n",
       "[2603 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# 使用glob模块找到所有的csv文件\n",
    "csv_files = glob.glob(\"data/output_hanlp+worldcloud/csv/*.csv\")\n",
    "# 读取每个csv文件并将它们存储在一个列表中\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "# 使用concat函数合并所有的DataFrame\n",
    "df_combine = pd.concat(dfs, ignore_index=True)\n",
    "df_combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine with the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine[\"filename_noindex\"] = df_combine[\"filename\"].str.split(\"_\").apply(lambda x: \"_\".join(x[1:]))\n",
    "display(df_combine)\n",
    "\n",
    "df_info = pd.read_csv(\"data/output_hanlp+worldcloud/round3_info.csv\")\n",
    "display(df_info.head(5))\n",
    "df_info_select = df_info[[\"访谈编号\",\"性别\",\"出生年份\",\"关内关外\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_combine, df_info_select, left_on='filename_noindex', right_on='访谈编号', how='left')\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only remove shenzen \n",
    "df_merge_remove_sz = df_merge.copy(deep=True)\n",
    "df_merge_remove_sz = df_merge_remove_sz.loc[(df_merge_remove_sz[\"location_name\"]!= \"深圳\")&(df_merge_remove_sz[\"location_name\"]!= \"深圳市\"),:]\n",
    "display(df_merge_remove_sz)\n",
    "\n",
    "# ## remove district as well as shenzhen\n",
    "# df_merge_remove_sz_district = df_merge_remove_sz.copy(deep=True)\n",
    "# df_merge_remove_sz_district = df_merge_remove_sz_district.loc[(df_merge_remove_sz_district[\"location_name\"]!= \"罗湖区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"福田区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"南山区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"宝安区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"龙岗区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"盐田区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"龙华区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"坪山区\")&(df_merge_remove_sz_district[\"location_name\"]!= \"光明区\")&\n",
    "# (df_merge_remove_sz_district[\"location_name\"]!= \"罗湖\")&(df_merge_remove_sz_district[\"location_name\"]!= \"福田\")&(df_merge_remove_sz_district[\"location_name\"]!= \"南山\")&(df_merge_remove_sz_district[\"location_name\"]!= \"宝安\")&(df_merge_remove_sz_district[\"location_name\"]!= \"龙岗\")&(df_merge_remove_sz_district[\"location_name\"]!= \"盐田\")&(df_merge_remove_sz_district[\"location_name\"]!= \"龙华\")&(df_merge_remove_sz_district[\"location_name\"]!= \"坪山\")&(df_merge_remove_sz_district[\"location_name\"]!= \"光明\"),:]\n",
    "# display(df_merge_remove_sz_district)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_list = [\"南山区\",\"南山\",\"坪山\",\"坪山区\",\"光明\",\"光明区\",\"盐田\",\"盐田区\",\"龙华\",\"龙华区\",\"龙岗\",\"龙岗区\",\"福田\",\"福田区\",\"罗湖\",\"罗湖区\",\"宝安\",\"宝安区\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_merge_remove_sz.groupby(\"location_name\").agg({\"count\":\"sum\"}).reset_index()\n",
    "display(df_all.sort_values(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_freq_all = list(df_all.itertuples(index=False, name=None))\n",
    "pd.DataFrame(noun_freq_all, columns=[\"word\",\"freq\"]).to_csv(\"data/output_hanlp+worldcloud/wordcloud/corresponding_table/all.csv\", index=False)\n",
    "# gen_stylecloud(\n",
    "# \ttext = dict(noun_freq_all),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \tpalette='colorbrewer.diverging.Spectral_11',\n",
    "# \t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \toutput_name='data/output_hanlp+worldcloud/wordcloud/all.png',       # 必要参数，保存词云图的路径\n",
    "# \tbackground_color='black',\n",
    "# \t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \t# icon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \ticon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \tcollocations = False,\n",
    "# \tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "#  \t# custom_stopwords=my_stop_list,\n",
    "# \trandom_state = 42\n",
    "# )\n",
    "\n",
    "# ## remove district\n",
    "# gen_stylecloud(\n",
    "# \ttext = dict(noun_freq_all),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \tpalette='colorbrewer.diverging.Spectral_11',\n",
    "# \t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \toutput_name='data/output_hanlp+worldcloud/wordcloud/all_rm_district.png',       # 必要参数，保存词云图的路径\n",
    "# \tbackground_color='black',\n",
    "# \t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \t# icon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \ticon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \tcollocations = False,\n",
    "# \tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "#  \tcustom_stopwords=my_stop_list,\n",
    "# \trandom_state = 42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性别（男-女）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把性别为 “一男一女” 转化为 “男” 或者 “女”，即保留两份数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = df_merge_remove_sz.copy(deep=True)\n",
    "df_gender_split = df_gender[df_gender['性别'] == '一男一女'].copy()\n",
    "\n",
    "# 创建两个副本，一个性别为“男”，另一个为“女”\n",
    "df_male = df_gender_split.copy()\n",
    "df_female = df_gender_split.copy()\n",
    "df_male['性别'] = '男'\n",
    "df_female['性别'] = '女'\n",
    "\n",
    "# concat函数将三个数据框连接在一起，删除性别为“一男一女”的数据\n",
    "df_gender = pd.concat([df_gender, df_male, df_female])\n",
    "df_gender = df_gender[df_gender['性别'] != '一男一女']\n",
    "df_gender.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(df_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_preprocess = df_gender.groupby([\"性别\",\"location_name\"]).agg({\"count\":\"sum\"}).reset_index()\n",
    "display(df_gender_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_freq_man = list(df_gender_preprocess.loc[df_gender_preprocess[\"性别\"]==\"男\",[\"location_name\",\"count\"]].itertuples(index=False, name=None))\n",
    "\n",
    "pd.DataFrame(noun_freq_man,columns=[\"word\",\"freq\"]).to_csv(\"data/output_hanlp+worldcloud/wordcloud/corresponding_table/gender_man.csv\", index=False)\n",
    "\n",
    "\n",
    "# gen_stylecloud(\n",
    "# \ttext = dict(noun_freq_man),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \tpalette='colorbrewer.diverging.Spectral_11',\n",
    "# \t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \toutput_name='data/output_hanlp+worldcloud/wordcloud/gender_man.png',       # 必要参数，保存词云图的路径\n",
    "# \tbackground_color='black',\n",
    "# \t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \ticon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t# icon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \tcollocations = False,\n",
    "# \tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "#  \t# custom_stopwords=my_stop_list,\n",
    "# \trandom_state = 42\n",
    "# )\n",
    "\n",
    "# ## remove district\n",
    "# gen_stylecloud(\n",
    "# \ttext = dict(noun_freq_man),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \tpalette='colorbrewer.diverging.Spectral_11',\n",
    "# \t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \toutput_name='data/output_hanlp+worldcloud/wordcloud/gender_man_rm_district.png',       # 必要参数，保存词云图的路径\n",
    "# \tbackground_color='black',\n",
    "# \t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \ticon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t# icon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \tcollocations = False,\n",
    "# \tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "#  \tcustom_stopwords=my_stop_list,\n",
    "# \trandom_state = 42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_freq_woman = list(df_gender_preprocess.loc[df_gender_preprocess[\"性别\"]==\"女\",[\"location_name\",\"count\"]].itertuples(index=False, name=None))\n",
    "\n",
    "pd.DataFrame(noun_freq_woman,columns=[\"word\",\"freq\"]).to_csv(\"data/output_hanlp+worldcloud/wordcloud/corresponding_table/gender_woman.csv\", index=False)\n",
    "\n",
    "\n",
    "# gen_stylecloud(\n",
    "# \ttext = dict(noun_freq_woman),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \t# palette='colorbrewer.diverging.Spectral_11',\n",
    "# \t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \toutput_name='data/output_hanlp+worldcloud/wordcloud/gender_woman.png',       # 必要参数，保存词云图的路径\n",
    "# \tbackground_color='white',\n",
    "# \t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \ticon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t# icon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \tcollocations = False,\n",
    "# \tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "#  \t# custom_stopwords=my_stop_list,\n",
    "# \trandom_state = 42\n",
    "# )\n",
    "\n",
    "# ## remove district\n",
    "# gen_stylecloud(\n",
    "# \ttext = dict(noun_freq_woman),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \t# palette='colorbrewer.diverging.Spectral_11',\n",
    "# \t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \toutput_name='data/output_hanlp+worldcloud/wordcloud/gender_woman_rm_district.png',       # 必要参数，保存词云图的路径\n",
    "# \tbackground_color='white',\n",
    "# \t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \ticon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t# icon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \tcollocations = False,\n",
    "# \tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "#  \tcustom_stopwords=my_stop_list,\n",
    "# \trandom_state = 42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 年龄（70年代，80年代、90年代、20世纪）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理 一份访谈中出现两个出生年份的 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df_merge_remove_sz.copy(deep=True)\n",
    "df_age_split = df_age.loc[df_age[\"出生年份\"].apply(lambda x: len(x)) >4,:].copy()\n",
    "df_age_split_cl = df_age_split.loc[df_age_split[\"出生年份\"] == \"陈女士1978 李先生1976\",:].copy(deep = True)\n",
    "df_age_split_dw = df_age_split.loc[df_age_split[\"出生年份\"] == \"邓先生1991，王女士1968\",:].copy(deep = True)\n",
    "display(df_age_split_cl.head(2))\n",
    "display(df_age_split_dw.head(2))\n",
    "\n",
    "df_age_split_cl_1978 = df_age_split_cl.copy(deep=True)\n",
    "df_age_split_cl_1978['出生年份'] = '1978'\n",
    "df_age_split_cl_1976 = df_age_split_cl.copy(deep=True)\n",
    "df_age_split_cl_1976['出生年份'] = '1976'\n",
    "\n",
    "df_age_split_dw_1991 = df_age_split_dw.copy(deep=True)\n",
    "df_age_split_dw_1991['出生年份'] = '1991'\n",
    "df_age_split_dw_1968 = df_age_split_dw.copy(deep=True)\n",
    "df_age_split_dw_1968['出生年份'] = '1968'\n",
    "\n",
    "df_age = pd.concat([df_age, df_age_split_cl_1978, df_age_split_cl_1976, df_age_split_dw_1991, df_age_split_dw_1968])\n",
    "df_age = df_age.loc[df_age[\"出生年份\"].apply(lambda x: len(x)) == 4,:].copy()\n",
    "df_age.reset_index(drop=True, inplace=True)\n",
    "display(df_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将出生年份离散化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age[\"age_group\"] = pd.cut(df_age[\"出生年份\"].astype(int), bins=[1940, 1970, 1980, 1990, 2000, 2010], labels=[\"leq70s\",\"70s\", \"80s\", \"90s\", \"00s\"],right=False)\n",
    "df_age_preprocess = df_age.groupby([\"age_group\",\"location_name\"]).agg({\"count\":\"sum\"}).reset_index()\n",
    "df_age_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_age(my_str):\n",
    "\tnoun_freq_tmp = list(df_age_preprocess.loc[df_age_preprocess[\"age_group\"]==my_str,[\"location_name\",\"count\"]].itertuples(index=False, name=None))\n",
    "\tpd.DataFrame(noun_freq_tmp,columns=[\"word\",\"freq\"]).to_csv(f\"data/output_hanlp+worldcloud/wordcloud/corresponding_table/age_{my_str}.csv\", index=False)\n",
    "\n",
    "# \tgen_stylecloud(\n",
    "# \t\ttext = dict(noun_freq_tmp),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \t\tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \t\tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \t\tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \t\tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \t\tpalette='colorbrewer.diverging.Spectral_11',\n",
    "# \t\t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \t\toutput_name=f'data/output_hanlp+worldcloud/wordcloud/age_{my_str}.png',       # 必要参数，保存词云图的路径\n",
    "# \t\tbackground_color='black',\n",
    "# \t\t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \t\t# icon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t\ticon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \t\tcollocations = False,\n",
    "# \t\tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "# \t\t# custom_stopwords=my_stop_list,\n",
    "# \t\trandom_state = 42\n",
    "# \t)\n",
    "\n",
    "# \t## remove district\n",
    "# \tgen_stylecloud(\n",
    "# \t\ttext = dict(noun_freq_tmp),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \t\tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \t\tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \t\tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \t\tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \t\t# palette='colorbrewer.diverging.Spectral_11',\n",
    "# \t\t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \t\toutput_name=f'data/output_hanlp+worldcloud/wordcloud/age_{my_str}_rm_district.png',       # 必要参数，保存词云图的路径\n",
    "# \t\tbackground_color='white',\n",
    "# \t\t# icon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \t\t# icon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t\ticon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \t\tcollocations = False,\n",
    "# \t\tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "# \t\tcustom_stopwords=my_stop_list,\n",
    "# \t\trandom_state = 42\n",
    "# \t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_age(\"leq70s\")\n",
    "word_cloud_age(\"70s\")\n",
    "word_cloud_age(\"80s\")\n",
    "word_cloud_age(\"90s\")\n",
    "word_cloud_age(\"00s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 活动区域（全关内、全关外、关内+关外）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关内-0：福田 罗湖 南山 盐田\n",
    "\n",
    "关外-1：龙岗 龙华 光明 宝安 坪山\n",
    "\n",
    "关内+关外-2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_area = df_merge_remove_sz.copy(deep=True)\n",
    "df_area_preprocess = df_area.groupby([\"关内关外\",\"location_name\"]).agg({\"count\":\"sum\"}).reset_index()\n",
    "df_area_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_area(my_str):\n",
    "\tnoun_freq_tmp = list(df_area_preprocess.loc[df_area_preprocess[\"关内关外\"]==my_str,[\"location_name\",\"count\"]].itertuples(index=False, name=None))\n",
    "\tif my_str == 0:\n",
    "\t\tmy_str = \"关内\"\n",
    "\tif my_str == 1:\n",
    "\t\tmy_str = \"关外\"\n",
    "\tif my_str == 2:\n",
    "\t\tmy_str = \"关内关外\"\n",
    "  \n",
    "\tpd.DataFrame(noun_freq_tmp,columns=[\"word\",\"freq\"]).to_csv(f\"data/output_hanlp+worldcloud/wordcloud/corresponding_table/area_{my_str}.csv\", index=False)\n",
    "\t\n",
    "# \tgen_stylecloud(\n",
    "# \t\ttext = dict(noun_freq_tmp),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \t\tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \t\tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \t\tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \t\tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \t\tpalette='colorbrewer.diverging.Spectral_11',\n",
    "# \t\t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \t\toutput_name=f'data/output_hanlp+worldcloud/wordcloud/area_{my_str}.png',       # 必要参数，保存词云图的路径\n",
    "# \t\tbackground_color='black',\n",
    "# \t\ticon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \t\t# icon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t\t# icon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \t\tcollocations = False,\n",
    "# \t\tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "# \t\t# custom_stopwords=my_stop_list,\n",
    "# \t\trandom_state = 42\n",
    "# \t)\n",
    "\n",
    "# \t## remove district\n",
    "# \tgen_stylecloud(\n",
    "# \t\ttext = dict(noun_freq_tmp),                    # {(word1, freq1),(word2, freq2), …}\n",
    "# \t\tsize=(1500,1500),         # 词云图的长宽，设置更大的数字可以增加成图的分辨率，但代码运行时间会随之增加\n",
    "# \t\tmax_words=1000,          # 词云图中的最大词语数量\n",
    "# \t\tmax_font_size=300,      # 词云图中字号的最大值\n",
    "# \t\tfont_path=r'C:\\Windows\\Fonts\\SimHei.ttf', # 字体是必要的参数，否则中文会显示异常\n",
    "# \t\t# palette='colorbrewer.diverging.Spectral_11',\n",
    "# \t\t# colors=['#ecf0f1', '#3498db', '#e74c3c'],                # 不想用palette可以自定义颜色\n",
    "# \t\toutput_name=f'data/output_hanlp+worldcloud/wordcloud/area_{my_str}_rm_district.png',       # 必要参数，保存词云图的路径\n",
    "# \t\tbackground_color='white',\n",
    "# \t\ticon_name= \"fas fa-comment-alt\" , # comment图标\n",
    "# \t\t# icon_name= \"fas fa-circle\" ,   # 圆形图标\n",
    "# \t\t# icon_name = \"fas fa-square-full\",  # 正方形图标\n",
    "# \t\tcollocations = False,\n",
    "# \t\tinvert_mask=False,                 # 形状反转（在画布里icon之外的地方绘图）\n",
    "# \t\tcustom_stopwords=my_stop_list,\n",
    "# \t\trandom_state = 42\n",
    "# \t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_area(0)\n",
    "word_cloud_area(1)\n",
    "word_cloud_area(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
