{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hanlp_restful import HanLPClient\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = os.listdir(\"data/Interview transcripts-round 3\")\n",
    "file_list = ['1_SZUZJY_来深33年邹先生.docx',\n",
    " '1_SZUZJY_深圳本地人林女士.docx',\n",
    " '2_JNU LLW_DB.docx',\n",
    " '2_JNU LLW_LXS.docx',\n",
    " '2_JNU LLW_RRAY.docx',\n",
    " '2_JNU LLW_SYJZ.docx',\n",
    " '2_JNU LLW_XBB.docx',\n",
    " '3_SZTUWWX_李先生.docx',\n",
    " '3_SZTUWWX_许女士.docx',\n",
    " '3_SZTUWWX_谭女士霍先生.docx',\n",
    " '3_SZTUWWX_邓先生王女士.docx',\n",
    " '3_SZTUWWX_陈女士.docx',\n",
    " '4_UT_LJY_UU.docx',\n",
    " '4_UT_LJY_WW.docx',\n",
    " '4_UT_LJY_XW.docx',\n",
    " '5_CS+刘阿姨.docx',\n",
    " '5_CS+尤老师.docx',\n",
    " '5_CS+钟老师.docx',\n",
    " '5_CS+黄先生.docx',\n",
    " '6_szulys_nw.docx',\n",
    " '6_szulys_乔麦.docx',\n",
    " '7_SZUF_LT.docx',\n",
    " '7_SZUF_坚.docx',\n",
    " '7_SZUF_大花.docx',\n",
    " '8_hkust_wdq_q.docx',\n",
    " '8_hkust_wdq_yyq.docx',\n",
    " '9_SZUCJH_发哥.docx',\n",
    " '9_SZUCJH_小贾.docx',\n",
    " '9_SZUCJH_小陈.docx',\n",
    " '9_SZUCJH_老陈.docx',\n",
    " '10_SZTU-lx_何爷爷.docx',\n",
    " '10_SZTU-lx_凹凸曼&李先生.docx',\n",
    " '11_SZUH_LEI.docx',\n",
    " '11_SZUH_YUNWEI.docx',\n",
    " '12_SZUL_Roson.docx',\n",
    " '12_SZUL_夏天.docx',\n",
    " '12_SZUL_张律师.docx',\n",
    " '12_SZUL_美丽.docx',\n",
    " '12_SZUL_花生.docx',\n",
    " '13_SZTULJK_HSF.docx',\n",
    " '13_SZTULJK_HYZ.docx',\n",
    " '13_SZTULJK_LDH.docx',\n",
    " '13_SZTULJK_LJP.docx',\n",
    " '14_SZUlsr-jiachunxia.docx',\n",
    " '14_SZUlsr-wuxin.docx',\n",
    " '15_PKU+DYE_Suning.docx',\n",
    " '15_PKU+DYE_Xin.docx',\n",
    " '15_PKU+DYE_Xuan.docx',\n",
    " '17_SCNUC_小于先生.docx',\n",
    " '17_SCNUC_小于女士.docx',\n",
    " '18_SZU_XZA_HE.docx',\n",
    " '19_sustech_qk_He.docx',\n",
    " '19_sustech_qk_meng.docx',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HanLP = HanLPClient('https://www.hanlp.com/api', auth='你申请到的auth')  # auth需要申请\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth=\"NTc3MkBiYnMuaGFubHAuY29tOnV6R0xMS05pblB3c29CZE4=\", language='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize(text: Union[str, List[str]], coarse: Optional[bool] = None, language=None) → List[List[str]]\n",
    "# seg = HanLP.tokenize(text)\n",
    "# seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return ''.join(full_text)\n",
    "\n",
    "# def get_location_from_one_hanlp(my_filepath):\n",
    "#     text = read_docx(my_filepath)\n",
    "#     # display(text)\n",
    "\n",
    "#     doc = HanLP(text, tasks='ner/pku', language='zh')\n",
    "#     my_ner_list = doc['ner/pku']\n",
    "\n",
    "#     my_filename = my_filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "#     return my_ner_list, my_filename\n",
    "\n",
    "def split_text(text, max_length=15000):\n",
    "    # 使用\\n分割文本为句子列表\n",
    "    sentences = text.split('\\n')\n",
    "    # display(len(sentences))\n",
    "    parts = []\n",
    "    current_part = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # 检查加入当前句子是否会超过最大长度\n",
    "        if len(current_part) + len(sentence) + 1 > max_length:  # +1 是为了计算加入的换行符\n",
    "            # 如果会超过，当前部分结束，开始新的部分\n",
    "            parts.append(current_part)\n",
    "            current_part = sentence  # 开始新的部分，当前句子是新部分的第一句\n",
    "        else:\n",
    "            # 如果不会超过，将当前句子加入当前部分\n",
    "            if current_part:  # 如果当前部分不为空，先加入换行符\n",
    "                current_part += '\\n'\n",
    "            current_part += sentence\n",
    "\n",
    "    # 循环结束后，将最后一部分（如果有）加入到部分列表中\n",
    "    if current_part:\n",
    "        parts.append(current_part)\n",
    "\n",
    "    return parts\n",
    "\n",
    "def get_location_from_one_hanlp(my_filepath):\n",
    "    text = read_docx(my_filepath)\n",
    "    # print(text)\n",
    "\n",
    "    # 检查文本长度并分割\n",
    "    # display(len(text.split(\"\\n\")))\n",
    "    texts = split_text(text) if len(text) > 15000 else [text]\n",
    "    # 初始化空列表来存储所有部分的结果\n",
    "    all_ner_lists = []\n",
    "    # display(len(texts))\n",
    "    # display(len(texts[0].split(\"\\n\")))\n",
    "\n",
    "    # 对每个文本部分调用 API 并合并结果\n",
    "    for part_text in texts:\n",
    "        doc = HanLP(part_text, tasks='ner/pku', language='zh')\n",
    "        \n",
    "        # doc.pretty_print()\n",
    "        \n",
    "        my_ner_list = doc['ner/pku']\n",
    "        # display(len(my_ner_list))\n",
    "        all_ner_lists.extend(my_ner_list)\n",
    "\n",
    "    my_filename = my_filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    # display(len(all_ner_lists))\n",
    "    return all_ner_lists, my_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(my_filepath):\n",
    "    text = read_docx(my_filepath)\n",
    "    # print(len(text))\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"data/Interview transcripts-round 3/1_SZUZJY_来深33年邹先生.docx\"\n",
    "# range_list = [0, 2, 7, 12, 15, 19, 22, 27, 29, 31, 33, 35, 40, 45, 48, 51]\n",
    "# range_list = range(0, 55)\n",
    "# # range_list = [1]\n",
    "# for i in tqdm(range_list):\n",
    "#     my_filepath = f\"data/Interview transcripts-round 3/{file_list[i]}\"\n",
    "#     # tmp = split_text(read_docx(my_filepath))\n",
    "#     ner_list, filename = get_location_from_one_hanlp(my_filepath)\n",
    "#     # display(ner_list)\n",
    "#     with open(f\"data/output_hanlp+worldcloud/txt/{filename}.txt\",\"w\") as f:\n",
    "#         f.write(\"[\\n\")\n",
    "#         for item in ner_list:\n",
    "#             f.write(\"%s\\n\" % item)\n",
    "#         f.write(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine `hanlp_list` & `match_list` to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_list_todf(my_filename):\n",
    "    result = get_one_hanlp_list(my_filename) + get_one_match_list(my_filename)\n",
    "    # df = pd.DataFrame(result, columns=['location_name', 'value', 'method']).groupby(\"location_name\").agg({\"value\": lambda x: list(x)}).reset_index()\n",
    "    # df[\"count\"] = df[\"value\"].apply(lambda x: len(x))\n",
    "    df = pd.DataFrame(result, columns=['location_name', 'value', 'method'])\n",
    "    df[\"filename\"] = my_filename\n",
    "    df.to_csv(f\"data/output_hanlp_match/csv/{my_filename}.csv\", index=False,encoding=\"utf-16\",sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def get_one_hanlp_list(my_filename):\n",
    "    # with open(f\"data/output_hanlp/{my_filename}.txt\", \"r\") as f:\n",
    "    with open(f\"data/output_hanlp+worldcloud/txt/{my_filename}.txt\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        tmp_ner_list = [ast.literal_eval(i.strip()) for i in content[1:-1].strip().split(\"\\n\")]\n",
    "        result =[[item[0], (index, item[2],item[3]),\"hanlp\"] for index, sublist in enumerate(tmp_ner_list) for item in sublist if item[1] == 'ns']\n",
    "    return result\n",
    "\n",
    "def get_one_match_list(my_filename):\n",
    "    with open(f\"data/output_match/txt/{my_filename}.txt\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        tmp_ner_list = [ast.literal_eval(i.strip()) for i in content[1:-1].strip().split(\"\\n\")]\n",
    "        result =[[item[0], (index, item[2],item[3]),\"patch\"] for index, sublist in enumerate(tmp_ner_list) for item in sublist if item[1] == 'ns']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>value</th>\n",
       "      <th>method</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>深圳</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>hanlp</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>深圳</td>\n",
       "      <td>(1, 30, 31)</td>\n",
       "      <td>hanlp</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>深圳</td>\n",
       "      <td>(6, 10, 11)</td>\n",
       "      <td>hanlp</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>龙华工业区</td>\n",
       "      <td>(8, 12, 14)</td>\n",
       "      <td>hanlp</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>罗湖书城</td>\n",
       "      <td>(8, 33, 35)</td>\n",
       "      <td>hanlp</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>机场</td>\n",
       "      <td>(133, 0, 0)</td>\n",
       "      <td>patch</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>11号线</td>\n",
       "      <td>(134, 0, 0)</td>\n",
       "      <td>patch</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1号线</td>\n",
       "      <td>(134, 0, 0)</td>\n",
       "      <td>patch</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>邮局</td>\n",
       "      <td>(170, 0, 0)</td>\n",
       "      <td>patch</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>大剧院</td>\n",
       "      <td>(178, 0, 0)</td>\n",
       "      <td>patch</td>\n",
       "      <td>1_SZUZJY_来深33年邹先生</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    location_name        value method           filename\n",
       "0              深圳    (1, 2, 3)  hanlp  1_SZUZJY_来深33年邹先生\n",
       "1              深圳  (1, 30, 31)  hanlp  1_SZUZJY_来深33年邹先生\n",
       "2              深圳  (6, 10, 11)  hanlp  1_SZUZJY_来深33年邹先生\n",
       "3           龙华工业区  (8, 12, 14)  hanlp  1_SZUZJY_来深33年邹先生\n",
       "4            罗湖书城  (8, 33, 35)  hanlp  1_SZUZJY_来深33年邹先生\n",
       "..            ...          ...    ...                ...\n",
       "259            机场  (133, 0, 0)  patch  1_SZUZJY_来深33年邹先生\n",
       "260          11号线  (134, 0, 0)  patch  1_SZUZJY_来深33年邹先生\n",
       "261           1号线  (134, 0, 0)  patch  1_SZUZJY_来深33年邹先生\n",
       "262            邮局  (170, 0, 0)  patch  1_SZUZJY_来深33年邹先生\n",
       "263           大剧院  (178, 0, 0)  patch  1_SZUZJY_来深33年邹先生\n",
       "\n",
       "[264 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = combine_list_todf(\"1_SZUZJY_来深33年邹先生\")\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 55.44it/s]\n"
     ]
    }
   ],
   "source": [
    "range_list = range(len(file_list))\n",
    "\n",
    "for i in tqdm(range_list):\n",
    "    filename = file_list[i].split(\".\")[0]\n",
    "    combine_list_todf(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
